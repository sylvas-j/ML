{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe10e78-1442-4cc3-8e6c-5a40b3033e00",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b527271a-5235-4d3f-91d0-dc393d3eed55",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "683370a4-581b-4153-b916-f9c75aa7aee2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 15:14:11.721489: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-19 15:14:12.423968: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-19 15:14:12.424994: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-19 15:14:14.690279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/sylvas/anaconda3/envs/tensorflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sylvas/anaconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.12.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# import os\n",
    "# import json\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "# import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text  # A dependency of the preprocessing model\n",
    "# import tensorflow_addons as tfa\n",
    "from official.nlp import optimization\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2cf68fb-12bd-4fe8-8730-23145ee3b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import nltk, time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import collections, itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4127eafb-94c8-4dd1-98b7-98b78d448b2e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sylvas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sylvas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sylvas/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/sylvas/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /home/sylvas/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/sylvas/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download these NLTK packages\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9372d84f-5864-4d26-b11c-6b33849a25bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9528c699-45fd-4ae1-a0a4-da72f0ffb4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../datasets/sherloc/sherloc_court_cases_7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f148b9b-e6f8-4dc1-b371-7fac896f5cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>crime_types</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mr. Solomon Sauls ran an illegal enterprise wi...</td>\n",
       "      <td>money laundry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SummaryHarmony Gold Mine (Pty) Limited is a mi...</td>\n",
       "      <td>money laundry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SummaryThe three defendants were found guilty ...</td>\n",
       "      <td>money laundry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Johannes Erasmus van Staden was a Cape Town bu...</td>\n",
       "      <td>money laundry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Juan Hattingh was a young practicing attorney ...</td>\n",
       "      <td>money laundry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    crime_types  sentence\n",
       "0  Mr. Solomon Sauls ran an illegal enterprise wi...  money laundry         1\n",
       "1  SummaryHarmony Gold Mine (Pty) Limited is a mi...  money laundry         1\n",
       "2  SummaryThe three defendants were found guilty ...  money laundry         1\n",
       "3  Johannes Erasmus van Staden was a Cape Town bu...  money laundry         1\n",
       "4  Juan Hattingh was a young practicing attorney ...  money laundry         1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b676c564-f3fc-49b0-ac12-e991125b6480",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploration of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57e2138-e533-4f62-a6e2-f61453c1d7c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a86a6e-9082-45db-a50d-7efe36deabdb",
   "metadata": {},
   "source": [
    "#### Step 1: Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2452236b-1904-46d1-9e62-3d1ef37c421d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr. Solomon Sauls ran an illegal enterprise with the purpose of poaching and selling abalone (Haliotis midae) in South Africa.',\n",
       " 'A group of abalone divers worked under him supplying him with illegally harvested abalone.',\n",
       " 'The accused bribed officials from the Department of Agriculture, Forestry and Fisheries to prevent them from confiscating abalone and to buy back abalone already seized by the authorities.',\n",
       " 'The corrupt officials are facing charges in a separate trial.In March 2018, the police searched the accusedâ€™s house and encountered cash that the accused confessed to proceeds of his illegal activities.',\n",
       " 'In February 2021, the defendant pleaded guilty to 41 counts involving running an illegal enterprise, corruption, money laundering and possessing and transporting illegally harvested abalone.Factors aggravating the sentence were the seriousness of corrupting government officials, the engagement in the illegal abalone trade on a commercial scale and financial greed.The accused has been involved in similar crimes in the past and previously received prison sentences for his involvement with another illegal enterprise focusing on abalone poaching.In 2019, he was found responsible for running an illegal abalone business in a different area and was handed a 14-year sentence.In the current trial, Solomon Sauls was sentenced to 244 years imprisonment, running concurrently and resulting in an effective 18-year sentence.',\n",
       " 'Of those 18 years, 12 were determined to run concurrently to the previous sentence of 14 years.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nltk.sent_tokenize(data.text[0])\n",
    "#to print sentences\n",
    "# doc\n",
    "for sent in doc:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76213379-5134-4b6b-8c13-c2b498661fca",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 2: Word tokenization.\n",
    "- drop duplicate and nan if any\n",
    "- tokenize text\n",
    "- transform text into lower case\n",
    "- remove punctuatios\n",
    "- remove stopword (step 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7d1aae9-f1a0-476d-a460-78340136b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize the tweets\n",
    "def custom_tokenize(text):\n",
    "    \"\"\"Function that tokenizes text\"\"\"\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    if not text:\n",
    "        print('The text to be tokenized is a None type. Defaulting to blank string.')\n",
    "        text = ''\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f07acbe-501a-43d8-aad2-da973d634044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(data):\n",
    "    \"\"\"Function that cleans up the data into a shape that can be further used for modeling\"\"\"\n",
    "    # english = data[data['lang']=='en'] # extract only tweets in english language\n",
    "    data.drop_duplicates() # drop duplicate tweets\n",
    "    data['text'].dropna(inplace=True) # drop any rows with missing tweets\n",
    "    tokenized = data['text'].apply(custom_tokenize) # Tokenize tweets\n",
    "    lower_tokens = tokenized.apply(lambda x: [t.lower() for t in x]) # Convert tokens into lower case\n",
    "    alpha_only = lower_tokens.apply(lambda x: [t for t in x if t.isalpha()]) # Remove punctuations\n",
    "    no_stops = alpha_only.apply(lambda x: [t for t in x if t not in stopwords.words('english')]) # remove stop words\n",
    "    # no_stops.apply(lambda x: [x.remove(t) for t in x if t=='rt']) # remove acronym \"rt\"\n",
    "    return no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5dcaf66c-89f7-4fd1-a938-5a0dfe3df7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text_chunk = clean_up(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b1a277a9-bfeb-4aaa-a7dc-f060e17c5524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text_chunk.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e43c51-db0d-4b5a-854d-88b0d2ef963b",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190aa7a7-6ec3-4fe4-87dc-ac038cf6a723",
   "metadata": {},
   "source": [
    "### Create bag of words with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6644749c-2141-4985-910e-b7092a2e63a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3023958206176758"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "# Create a Dictionary from the tweets\n",
    "start = time.time()\n",
    "\n",
    "dictionary = Dictionary(cleaned_text_chunk)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "total = end-start\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3f5bf5b1-c414-4675-bc53-a7f3750ffe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your dictionary as text file\n",
    "from gensim.test.utils import get_tmpfile\n",
    "tmp_fname = get_tmpfile(\"dictionary\")\n",
    "dictionary.save_as_text(tmp_fname)\n",
    " \n",
    "# load your dictionary text file\n",
    "load_dict = corpora.Dictionary.load_from_text(tmp_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "eaa0e5c1-7bbd-4bd1-8a6e-f7d140a37bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/tmpv0mr26sg/dictionary'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4d21b0a0-c6db-4995-9387-75c135214801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21866464614868164"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create corpus for bag of words (token IDs of each word with their frequencies)\n",
    "corpus = cleaned_text_chunk.apply(lambda x: dictionary.doc2bow(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "582ba30b-4e46-490b-be91-3faa4a2921b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [(0, 8), (1, 4), (2, 1), (3, 1), (4, 1), (5, 1...\n",
       "1       [(3, 2), (6, 2), (12, 1), (15, 1), (32, 3), (3...\n",
       "2       [(0, 6), (1, 1), (2, 1), (3, 1), (7, 4), (23, ...\n",
       "3       [(1, 1), (12, 2), (15, 2), (23, 1), (29, 2), (...\n",
       "4       [(15, 2), (23, 1), (31, 1), (44, 2), (51, 3), ...\n",
       "                              ...                        \n",
       "1245    [(1, 3), (26, 2), (43, 1), (58, 1), (70, 1), (...\n",
       "1246    [(235, 1), (346, 1), (791, 1), (1521, 1), (176...\n",
       "1247    [(1, 1), (26, 1), (48, 1), (58, 1), (70, 1), (...\n",
       "1248    [(58, 1), (124, 1), (143, 1), (195, 1), (252, ...\n",
       "1249    [(1, 1), (26, 3), (27, 1), (37, 3), (43, 2), (...\n",
       "Name: text, Length: 1250, dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25070696-9abf-437c-8b7f-d0acac800c13",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8127ec8b-4856-4944-a004-369d6453bfae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dff \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_text\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "data.rename(columns={'text':'raw_text'}, inplace=True)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "de439c06-6a97-434b-b30c-34e8c49e7cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>crime_types</th>\n",
       "      <th>sentence</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mr. Solomon Sauls ran an illegal enterprise wi...</td>\n",
       "      <td>money laundry</td>\n",
       "      <td>1</td>\n",
       "      <td>[solomon, sauls, ran, illegal, enterprise, pur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SummaryHarmony Gold Mine (Pty) Limited is a mi...</td>\n",
       "      <td>money laundry</td>\n",
       "      <td>1</td>\n",
       "      <td>[summaryharmony, gold, mine, pty, limited, min...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_text    crime_types  sentence   \n",
       "0  Mr. Solomon Sauls ran an illegal enterprise wi...  money laundry         1  \\\n",
       "1  SummaryHarmony Gold Mine (Pty) Limited is a mi...  money laundry         1   \n",
       "\n",
       "                                                text  \n",
       "0  [solomon, sauls, ran, illegal, enterprise, pur...  \n",
       "1  [summaryharmony, gold, mine, pty, limited, min...  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([data,cleaned_text_chunk],axis=1)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a1b740c9-3239-448d-957f-4d609eec1f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'text':'tokenized_cleaned_text'}, inplace=True)\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9807d7e-4bca-40a0-b263-c46e38d0f084",
   "metadata": {},
   "source": [
    "## Step 4: Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d38d6a30-001c-4af1-85cc-5278ac2bf4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0b8550da-c8e9-4756-b96a-d8e90e207ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4a49b31e-043c-460b-a3d6-3807cf931fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.417070150375366"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize tokens\n",
    "start = time.time()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['lemmatized'] = df['tokenized_cleaned_text'].apply(lambda x: [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in x])\n",
    "\n",
    "end = time.time()\n",
    "total = end-start\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7c88d048-4eee-4980-a5c1-de7d1f0d2ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>crime_types</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokenized_cleaned_text</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mr. Solomon Sauls ran an illegal enterprise wi...</td>\n",
       "      <td>money laundry</td>\n",
       "      <td>1</td>\n",
       "      <td>[solomon, sauls, ran, illegal, enterprise, pur...</td>\n",
       "      <td>[solomon, saul, ran, illegal, enterprise, purp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SummaryHarmony Gold Mine (Pty) Limited is a mi...</td>\n",
       "      <td>money laundry</td>\n",
       "      <td>1</td>\n",
       "      <td>[summaryharmony, gold, mine, pty, limited, min...</td>\n",
       "      <td>[summaryharmony, gold, mine, pty, limited, min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SummaryThe three defendants were found guilty ...</td>\n",
       "      <td>money laundry</td>\n",
       "      <td>1</td>\n",
       "      <td>[summarythe, three, defendants, found, guilty,...</td>\n",
       "      <td>[summarythe, three, defendant, found, guilty, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Johannes Erasmus van Staden was a Cape Town bu...</td>\n",
       "      <td>money laundry</td>\n",
       "      <td>1</td>\n",
       "      <td>[johannes, erasmus, van, staden, cape, town, b...</td>\n",
       "      <td>[johannes, erasmus, van, staden, cape, town, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Juan Hattingh was a young practicing attorney ...</td>\n",
       "      <td>money laundry</td>\n",
       "      <td>1</td>\n",
       "      <td>[juan, hattingh, young, practicing, attorney, ...</td>\n",
       "      <td>[juan, hattingh, young, practice, attorney, co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_text    crime_types  sentence   \n",
       "0  Mr. Solomon Sauls ran an illegal enterprise wi...  money laundry         1  \\\n",
       "1  SummaryHarmony Gold Mine (Pty) Limited is a mi...  money laundry         1   \n",
       "2  SummaryThe three defendants were found guilty ...  money laundry         1   \n",
       "3  Johannes Erasmus van Staden was a Cape Town bu...  money laundry         1   \n",
       "4  Juan Hattingh was a young practicing attorney ...  money laundry         1   \n",
       "\n",
       "                              tokenized_cleaned_text   \n",
       "0  [solomon, sauls, ran, illegal, enterprise, pur...  \\\n",
       "1  [summaryharmony, gold, mine, pty, limited, min...   \n",
       "2  [summarythe, three, defendants, found, guilty,...   \n",
       "3  [johannes, erasmus, van, staden, cape, town, b...   \n",
       "4  [juan, hattingh, young, practicing, attorney, ...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  [solomon, saul, ran, illegal, enterprise, purp...  \n",
       "1  [summaryharmony, gold, mine, pty, limited, min...  \n",
       "2  [summarythe, three, defendant, found, guilty, ...  \n",
       "3  [johannes, erasmus, van, staden, cape, town, b...  \n",
       "4  [juan, hattingh, young, practice, attorney, co...  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "eaa9cac4-97ae-49fc-86cb-21291b0417ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../datasets/sherloc/sherloc_court_cases_lemmatized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12625bc2-7123-47bb-966e-48bed5bedc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9edbb22-559c-45b3-88db-c03db8d2411c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mast\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# m= ast.literal_eval(f) # convert string to list just like json.loads()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dff[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens_back_to_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, ast\u001b[38;5;241m.\u001b[39mliteral_eval(l))) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m dff[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmatized\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      5\u001b[0m dff\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mast\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# m= ast.literal_eval(f) # convert string to list just like json.loads()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dff[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens_back_to_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mliteral_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m dff[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmatized\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      5\u001b[0m dff\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# lemmatized text back to tokenized\n",
    "import ast\n",
    "# m= ast.literal_eval(f) # convert string to list just like json.loads()\n",
    "dff['tokens_back_to_text'] = [' '.join(map(str, ast.literal_eval(l))) for l in dff['lemmatized']]\n",
    "dff.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1002b1d-7e00-4505-8444-3d7eb2630322",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.to_csv('../datasets/sherloc/token_back_to_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a0dd5-a883-493a-8699-e1d8fcc62a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
