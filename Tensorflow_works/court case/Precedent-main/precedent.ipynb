{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bc94a4-7f5f-495b-99a0-51d2c431fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import LongformerTokenizer, LongformerModel\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "import uuid\n",
    "\n",
    "from tqdm import tqdm\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5a0263-743f-4c0c-ac93-d2228fd7daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, out_dim=2, freeze_bert=False, dropout=0.2, n_hidden=50):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, n_hidden, out_dim\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.longformer = LongformerModel.from_pretrained('allenai/longformer-base-4096', return_dict=True, gradient_checkpointing=True)\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, global_attention):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "\n",
    "        outputs = self.longformer(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention)\n",
    "\n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1e3161-92ab-4379-8d9e-147a3d589fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "            print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "            # print('Device name:', torch.cuda.get_device_name(0))\n",
    "            # print('Device name:', torch.cuda.get_device_name(1))\n",
    "\n",
    "        else:\n",
    "            print('No GPU available, using the CPU instead.')\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "    def text_preprocessing(self, text):\n",
    "        \"\"\"\n",
    "        - Remove entity mentions (eg. '@united')\n",
    "        - Correct errors (eg. '&amp;' to '&')\n",
    "        @param    text (str): a string to be processed.\n",
    "        @return   text (Str): the processed string.\n",
    "        \"\"\"\n",
    "        # Remove '@name'\n",
    "        text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "        # Replace '&amp;' with '&'\n",
    "        text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "        # Remove trailing whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    # Create a function to tokenize a set of texts\n",
    "    def preprocessing_for_bert(self, data, tokenizer, max=4096):\n",
    "        \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "        @param    data (np.array): Array of texts to be processed.\n",
    "        @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "        @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                      tokens should be attended to by the model.\n",
    "        \"\"\"\n",
    "\n",
    "        # For every sentence...\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for sent in tqdm(data):\n",
    "            # `encode_plus` will:\n",
    "            #    (1) Tokenize the sentence\n",
    "            #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "            #    (3) Truncate/Pad sentence to max length\n",
    "            #    (4) Map tokens to their IDs\n",
    "            #    (5) Create attention mask\n",
    "            #    (6) Return a dictionary of outputs\n",
    "            encoded_sent = tokenizer.encode_plus(\n",
    "                text=self.text_preprocessing(sent),  # Preprocess sentence\n",
    "                add_special_tokens=True,  # Add `[CLS]` and `[SEP]`\n",
    "                max_length=max,  # Max length to truncate/pad\n",
    "                pad_to_max_length=True,  # Pad sentence to max length\n",
    "                # return_tensors='pt',           # Return PyTorch tensor\n",
    "                return_attention_mask=True,  # Return attention mask\n",
    "                truncation=True,\n",
    "            )\n",
    "\n",
    "            # Add the outputs to the lists\n",
    "            input_ids.append([encoded_sent.get('input_ids')])\n",
    "            attention_masks.append([encoded_sent.get('attention_mask')])\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "        return input_ids, attention_masks\n",
    "\n",
    "    def initialize_model(self, out_dim=2, epochs=4, train_dataloader=None, learning_rate=3e-5, dropout=0.2, n_hidden=50):\n",
    "        \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "        \"\"\"\n",
    "        # Instantiate Bert Classifier\n",
    "        model = BertClassifier(out_dim, dropout=dropout, n_hidden=n_hidden)\n",
    "\n",
    "        # MULTI GPU support:\n",
    "        # if torch.cuda.device_count() > 1:\n",
    "        #     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        #     model = nn.DataParallel(model)\n",
    "\n",
    "        # Tell PyTorch to run the model on GPU\n",
    "        model.to(self.device)\n",
    "\n",
    "        # Create the optimizer\n",
    "        optimizer = AdamW(model.parameters(),\n",
    "                          lr=learning_rate, # lr=5e-5,    # Default learning rate\n",
    "                          eps=1e-8    # Default epsilon value\n",
    "                          )\n",
    "\n",
    "        # Total number of training steps\n",
    "        total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "        # Set up the learning rate scheduler\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                    num_warmup_steps=0, # Default value\n",
    "                                                    num_training_steps=total_steps)\n",
    "        return model, optimizer, scheduler\n",
    "\n",
    "\n",
    "    def set_seed(self, seed_value=42):\n",
    "        \"\"\"Set seed for reproducibility.\n",
    "        \"\"\"\n",
    "        random.seed(seed_value)\n",
    "        np.random.seed(seed_value)\n",
    "        torch.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "\n",
    "    def train(self, model, train_dataloader, val_dataloader=None, test_dataloader=None, epochs=4, evaluation=False, loss_fn=False, optimizer=False, scheduler=False, binary=False):\n",
    "        \"\"\"Train the BertClassifier model.\n",
    "        \"\"\"\n",
    "        # Start training loop\n",
    "        print(\"Start training...\\n\")\n",
    "\n",
    "        unique_id = \"models/\" + uuid.uuid4().hex + '.pt'\n",
    "        print(unique_id)\n",
    "        stop_cnt = 0\n",
    "        best_loss = 100\n",
    "        for epoch_i in range(epochs):\n",
    "            # =======================================\n",
    "            #               Training\n",
    "            # =======================================\n",
    "            # Print the header of the result table\n",
    "            print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Val F1':^9} | {'Val Prec':^9} | {'Val Rec':^9} | {'Elapsed':^9}\")\n",
    "            print(\"-\" *105)\n",
    "\n",
    "            # Measure the elapsed time of each epoch\n",
    "            t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "            # Reset tracking variables at the beginning of each epoch\n",
    "            total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "            # Put the model into the training mode\n",
    "            model.train()\n",
    "\n",
    "            # For each batch of training data...\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                batch_counts += 1\n",
    "                # Load batch to GPU\n",
    "                b_input_ids, b_attn_mask, b_labels = tuple(t.to(self.device) for t in batch)\n",
    "\n",
    "                # Zero out any previously calculated gradients\n",
    "                model.zero_grad()\n",
    "\n",
    "                # Perform a forward pass. This will return logits.\n",
    "\n",
    "                b_input_ids = b_input_ids.squeeze()\n",
    "                b_attn_mask = b_attn_mask.squeeze()\n",
    "\n",
    "                global_attention_mask = torch.zeros(b_input_ids.shape, dtype=torch.long, device=self.device)\n",
    "                global_attention_mask[:, [0]] = 1\n",
    "\n",
    "                logits = model(input_ids=b_input_ids, attention_mask=b_attn_mask, global_attention=global_attention_mask)\n",
    "                # Compute loss and accumulate the loss values\n",
    "\n",
    "                if binary:\n",
    "                    loss_each = loss_fn(logits, b_labels)\n",
    "                else:\n",
    "                    loss_each = loss_fn(logits, b_labels.float())\n",
    "\n",
    "                loss = torch.mean(loss_each)\n",
    "                batch_loss += loss.detach().item()\n",
    "                total_loss += loss.detach().item()\n",
    "\n",
    "                # Perform a backward pass to calculate gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                # Update parameters and the learning rate\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                # Print the loss values and time elapsed for every 20 batches\n",
    "                if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                    # Calculate time elapsed for 20 batches\n",
    "                    time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                    # Print training results\n",
    "                    print(\n",
    "                        f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {'-':^9} | {'-':^9} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                    # Reset batch tracking variables\n",
    "                    batch_loss, batch_counts = 0, 0\n",
    "                    t0_batch = time.time()\n",
    "\n",
    "            # Calculate the average loss over the entire training data\n",
    "            avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "            print(\"-\" * 105)\n",
    "            # =======================================\n",
    "            #               Evaluation\n",
    "            # =======================================\n",
    "            if evaluation == True:\n",
    "                # After the completion of each training epoch, measure the model's performance\n",
    "                # on our validation set.\n",
    "                val_loss, val_accuracy, val_f1_micro, val_prec_micro, val_rec_micro, _, _, _ = self.evaluate(model, val_dataloader, loss_fn, binary)\n",
    "\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    stop_cnt = 0\n",
    "                    torch.save(model, unique_id)\n",
    "                else:\n",
    "                    stop_cnt += 1\n",
    "                    print(f\"No Improvement! Stop cnt {stop_cnt}\")\n",
    "                # Print performance over the entire training data\n",
    "                time_elapsed = time.time() - t0_epoch\n",
    "\n",
    "                print(\n",
    "                    f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | \"\n",
    "                    f\"{val_f1_micro:^9.2f} | {val_prec_micro:^9.2f} | {val_rec_micro:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "                print(\"-\" * 105)\n",
    "            print(\"\\n\")\n",
    "\n",
    "            if stop_cnt == 1:\n",
    "                print(f\"Early Stopping at {stop_cnt}\")\n",
    "                model = torch.load(unique_id)\n",
    "                break\n",
    "\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "        val_loss, val_accuracy, val_f1_micro, val_prec_micro, val_rec_micro, all_val_losses, _, _ = self.evaluate(model, val_dataloader, loss_fn, binary)\n",
    "        print(f\"Best val loss: {best_loss}, recorded val loss: {val_loss}\")\n",
    "        test_loss, test_accuracy, test_f1_micro, test_prec_micro, test_rec_micro, all_test_losses, all_preds, all_truths = self.evaluate(model, test_dataloader, loss_fn, binary)\n",
    "\n",
    "        # Print performance over the entire training data\n",
    "        print(\n",
    "            f\"{'Test Loss':^12} | {'Acc':^9} | {'F1':^9} | {'Precission':^9} | {'Recall':^9}\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"{test_loss:^12.6f} | {test_accuracy:^9.2f} | {test_f1_micro:^9.2f} | {test_prec_micro:^9.2f} | {test_rec_micro:^9.2f}\")\n",
    "\n",
    "        return val_loss, val_prec_micro, val_rec_micro, val_f1_micro, test_loss, test_prec_micro, test_rec_micro, test_f1_micro, unique_id, all_test_losses, all_preds, all_truths\n",
    "\n",
    "\n",
    "    def evaluate(self, model, val_dataloader, loss_fn, binary):\n",
    "        \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "        on our validation set.\n",
    "        \"\"\"\n",
    "        # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "        # the test time.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables\n",
    "        val_accuracy = []\n",
    "        val_loss = []\n",
    "\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        all_loss = []\n",
    "        all_truths = []\n",
    "\n",
    "        # For each batch in our validation set...\n",
    "        for batch in val_dataloader:\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(self.device) for t in batch)\n",
    "\n",
    "            # Compute logits\n",
    "            with torch.no_grad():\n",
    "                b_input_ids = b_input_ids.squeeze()\n",
    "                b_attn_mask = b_attn_mask.squeeze()\n",
    "\n",
    "                global_attention_mask = torch.zeros(b_input_ids.shape, dtype=torch.long, device=self.device)\n",
    "                global_attention_mask[:, [0]] = 1\n",
    "\n",
    "                logits = model(input_ids=b_input_ids, attention_mask=b_attn_mask, global_attention=global_attention_mask)\n",
    "\n",
    "            # Compute loss\n",
    "            if binary:\n",
    "                loss_each = loss_fn(logits, b_labels)\n",
    "            else:\n",
    "                loss_each = loss_fn(logits, b_labels.float())\n",
    "\n",
    "            all_loss += loss_each.detach().tolist()\n",
    "\n",
    "            loss = torch.mean(loss_each)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            # Get the predictions\n",
    "            if binary:\n",
    "                preds = torch.argmax(logits, dim=1).flatten()\n",
    "                # Calculate the accuracy rate\n",
    "                accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "            else:\n",
    "                preds = torch.round(torch.sigmoid(logits))\n",
    "                # Calculate the accuracy rate\n",
    "                accuracy = (preds == b_labels.float()).cpu().numpy().mean() * 100\n",
    "\n",
    "            val_accuracy.append(accuracy)\n",
    "\n",
    "            all_truths += b_labels.float().tolist()\n",
    "\n",
    "            if binary:\n",
    "                all_labels += b_labels.cpu().tolist()\n",
    "                all_preds += preds.cpu().tolist()\n",
    "            else:\n",
    "                all_labels += b_labels.cpu().float().tolist()\n",
    "                all_preds += preds.cpu().float().tolist()\n",
    "\n",
    "        # Compute the average accuracy and loss over the validation set.\n",
    "        avg_val_loss = np.mean(val_loss)\n",
    "        val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "        if binary:\n",
    "            val_f1 = f1_score(all_labels, all_preds, average=\"macro\") * 100\n",
    "            val_prec = precision_score(all_labels, all_preds, average=\"macro\") * 100\n",
    "            val_rec = recall_score(all_labels, all_preds, average=\"macro\") * 100\n",
    "        else:\n",
    "            val_f1 = f1_score(all_labels, all_preds, average=\"micro\") * 100\n",
    "            val_prec = precision_score(all_labels, all_preds, average=\"micro\") * 100\n",
    "            val_rec = recall_score(all_labels, all_preds, average=\"micro\") * 100\n",
    "\n",
    "        return avg_val_loss, val_accuracy, val_f1, val_prec, val_rec, all_loss, all_preds, all_truths\n",
    "\n",
    "    def get_data(self, src_path='ECHR/EN_train', binary=True, seq_len=100):\n",
    "\n",
    "        all_text = []\n",
    "        all_targets = []\n",
    "        non_cnt = 0\n",
    "        tru_cnt = 0\n",
    "        all_multi_targets = []\n",
    "\n",
    "        sizes = []\n",
    "\n",
    "        for item in tqdm(os.listdir(src_path)):\n",
    "            if item.endswith('.json'):\n",
    "                with open(os.path.join(src_path, item)) as json_file:\n",
    "                    data = json.load(json_file)\n",
    "\n",
    "                    text = \" \".join(data[\"precedent_facts\"])\n",
    "\n",
    "                    if len(text) != 0:\n",
    "                        text = [\" \".join(t.split()[:seq_len]) for t in data[\"precedent_facts\"]]\n",
    "                        text = \" \".join(text)\n",
    "\n",
    "                        all_text.append(text)\n",
    "\n",
    "                        violation = len(data[\"violated_articles\"]) != 0\n",
    "\n",
    "                        if violation:\n",
    "                            all_targets.append(1)\n",
    "                            all_multi_targets.append(data[\"violated_articles\"])\n",
    "                            non_cnt += 1\n",
    "                        else:\n",
    "                            all_targets.append(0)\n",
    "                            tru_cnt += 1\n",
    "                            all_multi_targets.append([\"0\"])\n",
    "\n",
    "                        sizes.append(len(text.split(\" \")))\n",
    "\n",
    "        if binary:\n",
    "            y = np.array(all_targets)\n",
    "        else:\n",
    "            y = np.array(all_multi_targets)\n",
    "\n",
    "        return np.array(all_text), y\n",
    "\n",
    "    def run(self, epochs=2, binary=True, batch_size=16, max_len=4096, lr=3e-5, dropout=0.2, n_hidden=50, seq_len=100, data_type=\"precedent_facts\"):\n",
    "\n",
    "        with open(\"pretokenized/\"+data_type+\"/tokenized_train.pkl\", \"rb\") as f:\n",
    "            train_inputs, train_masks, train_labels, train_ids = pickle.load(f)\n",
    "\n",
    "        with open(\"pretokenized/\"+data_type+\"/tokenized_val.pkl\", \"rb\") as f:\n",
    "            val_inputs, val_masks, val_labels, val_ids = pickle.load(f)\n",
    "\n",
    "        with open(\"pretokenized/\"+data_type+\"/tokenized_test.pkl\", \"rb\") as f:\n",
    "            test_inputs, test_masks, test_labels, test_ids = pickle.load(f)\n",
    "\n",
    "\n",
    "        if data_type == \"facts\" or data_type == \"arguments\":\n",
    "            train_inputs, train_masks = train_inputs[:, :, :512], train_masks[:, :, :512]\n",
    "            val_inputs, val_masks = val_inputs[:, :, :512], val_masks[:, :, :512]\n",
    "            test_inputs, test_masks = test_inputs[:, :, :512], test_masks[:, :, :512]\n",
    "        elif data_type == \"precedent_both\":\n",
    "            train_inputs, train_masks = train_inputs[:, :, :2560], train_masks[:, :, :2560]\n",
    "            val_inputs, val_masks = val_inputs[:, :, :2560], val_masks[:, :, :2560]\n",
    "            test_inputs, test_masks = test_inputs[:, :, :2560], test_masks[:, :, :2560]\n",
    "        else:\n",
    "            train_inputs, train_masks = train_inputs[:, :, :1024], train_masks[:, :, :1024]\n",
    "            val_inputs, val_masks = val_inputs[:, :, :1024], val_masks[:, :, :1024]\n",
    "            test_inputs, test_masks = test_inputs[:, :, :1024], test_masks[:, :, :1024]\n",
    "\n",
    "        out_dim = len(train_labels[1])\n",
    "        print(\"Classifying into: \", out_dim)\n",
    "        print(\"DONE Loading\")\n",
    "\n",
    "        # Create the DataLoader for our training set\n",
    "        train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "        # Create the DataLoader for our validation set\n",
    "        val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "        val_sampler = SequentialSampler(val_data)\n",
    "        val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "        # Create the DataLoader for our training set\n",
    "        test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "        test_sampler = SequentialSampler(test_data)\n",
    "        test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "        # Specify loss function\n",
    "        if binary:\n",
    "            loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "        else:\n",
    "            loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "        # self.set_seed(42)    # Set seed for reproducibility\n",
    "        classifier, optimizer, scheduler = self.initialize_model(out_dim=out_dim, epochs=epochs, train_dataloader=train_dataloader, learning_rate=lr, dropout=dropout, n_hidden=n_hidden)\n",
    "        val_loss, val_precission, val_recall, val_f1, test_loss, test_precission, test_recall, test_f1, model_name, all_test_losses, all_test_preds, all_test_truths = self.train(classifier, train_dataloader, val_dataloader,\n",
    "                                                                                                                                                test_dataloader, epochs=epochs, evaluation=True,\n",
    "                                                                                                                                                 loss_fn=loss_fn, optimizer=optimizer, scheduler=scheduler,\n",
    "                                                                                                                                                 binary=binary)\n",
    "\n",
    "        # val_loss, val_precission, val_recall, val_f1, test_loss, test_precission, test_recall, test_f1, model_name, all_test_losses = 1, 1, 1, 1, 1, 1, 1, 1, \"test\", [1, 2, 3, 4, 5]\n",
    "\n",
    "        self.log_saver(val_precission, val_recall, val_f1, test_precission, test_recall, test_f1, model_name, data_type, dropout, lr, batch_size, n_hidden, val_loss, test_loss, all_test_losses, all_test_preds,  all_test_truths, test_ids)\n",
    "\n",
    "\n",
    "\n",
    "        return val_loss, val_precission, val_recall, val_f1, test_precission, test_recall, test_f1, model_name\n",
    "\n",
    "\n",
    "    def log_saver(self, dev_precission, dev_recall, dev_f1, test_precission, test_recall, test_f1, model_name, model_type, dropout, lr, batch_size, n_hidden, dev_loss, test_loss, all_test_losses, all_test_preds,  all_test_truths, test_ids):\n",
    "\n",
    "\n",
    "        with open(\"models/\" + model_type + \"_preds.csv\", 'w') as loss_file:\n",
    "            writer = csv.writer(loss_file)\n",
    "            for t_id, loss, pred, truth in zip(test_ids, all_test_losses, all_test_preds, all_test_truths):\n",
    "                writer.writerow([t_id, loss, pred, truth])\n",
    "\n",
    "        csv_columns = [\"model_name\", \"model_type\", \"dropout\", \"lr\", \"batch_size\", \"n_hidden\", \"dev_precission\", \"dev_recall\", \"dev_f1\", \"test_precission\", \"test_recall\", \"test_f1\", \"dev_loss\", \"test_loss\"]\n",
    "        new_dict_data = [\n",
    "            {\"model_name\": model_name, \"model_type\": model_type, \"dropout\": dropout, \"lr\": lr, \"batch_size\": batch_size, \"n_hidden\": n_hidden, \"dev_precission\": dev_precission, \"dev_recall\": dev_recall, \"dev_f1\": dev_f1, \"test_precission\": test_precission, \"test_recall\":test_recall, \"test_f1\":test_f1, \"dev_loss\":dev_loss, \"test_loss\":test_loss}\n",
    "        ]\n",
    "\n",
    "        csv_file = \"models/\" + model_type + \"_results.csv\"\n",
    "\n",
    "        if os.path.isfile(csv_file):\n",
    "            try:\n",
    "                with open(csv_file, 'a') as csvfile:\n",
    "                    writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "                    for data in new_dict_data:\n",
    "                        writer.writerow(data)\n",
    "\n",
    "            except IOError:\n",
    "                print(\"I/O error\")\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                with open(csv_file, 'w') as csvfile:\n",
    "                    writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "                    writer.writeheader()\n",
    "                    for data in new_dict_data:\n",
    "                        writer.writerow(data)\n",
    "\n",
    "            except IOError:\n",
    "                print(\"I/O error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc50fa6-2d38-470f-bc5a-1b0b925edc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--seq_length\", type=int, default=512, required=False)\n",
    "    parser.add_argument(\"--max_length\", type=int, default=1024, required=False)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=16, required=False)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=3e-5, required=False)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.2, required=False)\n",
    "    parser.add_argument(\"--n_hidden\", type=float, default=50, required=False)\n",
    "    parser.add_argument(\"--data_type\", type=str, default=\"precedent_arguments\", required=False)\n",
    "    parser.add_argument(\"--bin\", dest='bin', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "\n",
    "    cl = Classifier()\n",
    "    cl.run(epochs=10, binary=args.bin, max_len=args.max_length, batch_size=args.batch_size,\n",
    "           lr=args.learning_rate, dropout=args.dropout, n_hidden=args.n_hidden, seq_len=args.seq_length, data_type=args.data_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e551e1d-4e67-4b3a-a6c5-20b56513e3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4ae727-c60e-43f9-808b-a98fb5598807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8308cf9-3417-42cd-b87d-303ebf82eed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587b19c-a855-401a-aca9-037fb01ad42b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56057a76-3a0b-441f-83fb-8ed55d09bc66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ef022-f004-493c-bbae-0ee77e1c7439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
