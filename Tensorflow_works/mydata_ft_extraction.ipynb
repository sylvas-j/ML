{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\FACULTY OF SCIENCE\\\\Documents\\\\Tensorflow_works'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openpyxl as op\n",
    "import xlrd\n",
    "import pyexcel\n",
    "import xlutils\n",
    "import xlwt as xl\n",
    "import xlsxwriter\n",
    "from pandas import ExcelWriter\n",
    "from xlutils.copy import copy\n",
    "from xlrd import open_workbook\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pathlib as pb\n",
    "from tensorflow_core.python.feature_column.feature_column_v2 import BucketizedColumn\n",
    "from tensorflow.python.keras.layers import Dense, Dropout\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "import secrets\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tsfel\n",
    "import zipfile\n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "sns.set()\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\FACULTY OF SCIENCE\\\\Documents\\\\Tensorflow_works')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                    X,Y,Z,TP\n",
       "0            0.472,0.17,5,0\n",
       "1     -0.205,-0.425,0.271,5\n",
       "2     -0.184,-0.866,0.265,5\n",
       "3     -0.166,-0.779,-0.13,5\n",
       "4      -0.184,-1.046,0.58,5\n",
       "...                     ...\n",
       "9532  -0.147,0.371,-0.065,5\n",
       "9533  -0.442,0.161,-0.057,5\n",
       "9534  -0.397,0.145,-0.035,5\n",
       "9535   -0.668,0.062,-0.03,6\n",
       "9536  -0.601,0.055,-0.024,4\n",
       "\n",
       "[9537 rows x 1 columns]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtraintxtfile = 'D:\\\\desktop files\\\\weka\\\\trainfile.txt'\n",
    "traintxtf = pd.read_table(xtraintxtfile)\n",
    "traintxtf\n",
    "\n",
    "#traindata = np.loadtxt('D:\\\\desktop files\\\\weka\\\\trainfile.txt')\n",
    "traindata1 = np.array(pd.read_csv('D:\\\\desktop files\\\\weka\\\\trainfile.txt', header=None, delimiter=' '))\n",
    "activity_labels = np.array(pd.read_csv('D:\\\\desktop files\\\\weka\\\\SOFTX_2020_1-master\\\\notebooks\\\\UCI HAR Dataset\\\\activity_labels.txt', header=None, delimiter=' '))[:,1]\n",
    "traintxtf.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I have to make means to format the data into float32/signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['X,Y,Z,TP'],\n",
       "       ['0.472,0.17,5,0'],\n",
       "       ['-0.205,-0.425,0.271,5'],\n",
       "       ...,\n",
       "       ['-0.397,0.145,-0.035,5'],\n",
       "       ['-0.668,0.062,-0.03,6'],\n",
       "       ['-0.601,0.055,-0.024,4']], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata1\n",
    "#train = traindata1.astype(np.float)\n",
    "#train = ', '.join(traindata1)\n",
    "#train = np.fromstring(traindata1, dtype = np.float, sep =', ')\n",
    "#train = np.asarray(traindata1, dtype = np.float64, order ='C') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I think is time to run future_column command here on 'traintxtf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          X      Y      Z  TP\n",
      "0     0.472  0.170  5.000   0\n",
      "1    -0.205 -0.425  0.271   5\n",
      "2    -0.184 -0.866  0.265   5\n",
      "3    -0.166 -0.779 -0.130   5\n",
      "4    -0.184 -1.046  0.580   5\n",
      "...     ...    ...    ...  ..\n",
      "9531 -0.163  0.412 -0.001   5\n",
      "9532 -0.147  0.371 -0.065   5\n",
      "9533 -0.442  0.161 -0.057   5\n",
      "9534 -0.397  0.145 -0.035   5\n",
      "9535 -0.668  0.062 -0.030   6\n",
      "\n",
      "[9536 rows x 4 columns]\r\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('D:\\\\desktop files\\\\weka\\\\csvsample__1noG.csv')\n",
    "#data.head(4)\n",
    "tf.print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6102 train data\n",
      "1526 val data\n",
      "1908 test data\n"
     ]
    }
   ],
   "source": [
    "# this will split the data into trainn \n",
    "train, test = train_test_split(data, test_size = 0.2)\n",
    "train, val = train_test_split(train, test_size = 0.2)\n",
    "print(len(train), 'train data')\n",
    "print(len(val), 'val data')\n",
    "print(len(test), 'test data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the data to dataset\n",
    "def df_to_dataset(data,shuffle=True, batch_size=2000):\n",
    "    data=data.copy()\n",
    "    labels = data.pop('TP')\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(data), labels))\n",
    "    if shuffle:\n",
    "        ds=ds.shuffle(buffer_size=len(data))\n",
    "    ds=ds.batch(batch_size)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the dataframe to dataset via function created\n",
    "train_ds = df_to_dataset(train, batch_size = 2000)\n",
    "val_ds = df_to_dataset(val, batch_size = 500)\n",
    "test_ds = df_to_dataset(test,shuffle= False, batch_size = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['X', 'Y', 'Z']\n",
      "A batch of x: tf.Tensor([-0.39  -0.094 -0.137 ...  0.375  0.526  0.193], shape=(2000,), dtype=float64)\n",
      "A batch of tp: tf.Tensor([4 5 5 ... 5 5 5], shape=(2000,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#  A check of the dataset\n",
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "    print('Every feature:', list(feature_batch.keys()))\n",
    "    print('A batch of x:', feature_batch['X'])\n",
    "    print('A batch of tp:', label_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to play around with feature_column\n",
    "def demo(feature_column):\n",
    "    feature_layer=tf.keras.layers.DenseFeatures(feature_column)\n",
    "    print(feature_layer(data).numpy())\n",
    "    #print(feature_layer(example_batch).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "feature_layer_inputs = {}\n",
    "# numeric cols\n",
    "for header in ['X', 'Y', 'Z']:\n",
    "    feature_columns.append(feature_column.numeric_column(header))\n",
    "    feature_layer_inputs[header] = tf.keras.Input(shape=(1,), name=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column x bucket\n"
     ]
    }
   ],
   "source": [
    "x = feature_column.numeric_column('X')\n",
    "#point = feature_column.categorical_column_with_vocabulary_list('point', df['point'].unique())\n",
    "x_bucket=feature_column.bucketized_column(x, boundaries = [1,2,3,4])\n",
    "print('column x bucket')\n",
    "feature_columns.append(x_bucket)\n",
    "#demo(x_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column y bucket\n"
     ]
    }
   ],
   "source": [
    "y = feature_column.numeric_column('Y')\n",
    "#point = feature_column.categorical_column_with_vocabulary_list('point', df['point'].unique())\n",
    "y_bucket=feature_column.bucketized_column(y, boundaries = [1,2,3,4,5])\n",
    "print('column y bucket')\n",
    "feature_columns.append(y_bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column z bucket\n"
     ]
    }
   ],
   "source": [
    "z = feature_column.numeric_column('Z')\n",
    "#point = feature_column.categorical_column_with_vocabulary_list('point', df['point'].unique())\n",
    "z_bucket=feature_column.bucketized_column(z, boundaries = [1,2,3,4])\n",
    "print('column z bucket')\n",
    "feature_columns.append(z_bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#t = feature_column.categorical_column_with_vocabulary_list('TP', data['TP'].unique())\n",
    "#t_ind=feature_column.indicator_column(t)\n",
    "#print('column t indicated')\n",
    "#feature_columns.append(t_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_bucket' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-3072470d7888>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcrossed_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrossed_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_bucket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhash_bucket_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcrossed_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindicator_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrossed_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfeature_columns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrossed_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'crossed feature'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_bucket' is not defined"
     ]
    }
   ],
   "source": [
    "crossed_feature = feature_column.crossed_column([x_bucket, y], hash_bucket_size=2000)\n",
    "crossed_feature = feature_column.indicator_column(crossed_feature)\n",
    "feature_columns.append(crossed_feature)\n",
    "print('crossed feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "#feature_layer_outputs = feature_layer(feature_layer_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential([\n",
    "    feature_layer,\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(.1),\n",
    "    layers.Dense(1)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 4 steps, validate for 4 steps\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 4s 890ms/step - loss: -1.2136 - accuracy: 0.0000e+00 - val_loss: -2.5937 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 75ms/step - loss: -3.3328 - accuracy: 0.0000e+00 - val_loss: -4.7909 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 77ms/step - loss: -5.6662 - accuracy: 0.0000e+00 - val_loss: -7.3161 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 77ms/step - loss: -8.3184 - accuracy: 0.0000e+00 - val_loss: -10.1709 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 73ms/step - loss: -11.4598 - accuracy: 0.0000e+00 - val_loss: -13.9105 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 73ms/step - loss: -15.3398 - accuracy: 0.0000e+00 - val_loss: -18.2236 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 77ms/step - loss: -20.1414 - accuracy: 0.0000e+00 - val_loss: -23.7744 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 76ms/step - loss: -26.0161 - accuracy: 0.0000e+00 - val_loss: -30.6955 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 76ms/step - loss: -33.4351 - accuracy: 0.0000e+00 - val_loss: -39.3794 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 78ms/step - loss: -42.5985 - accuracy: 0.0000e+00 - val_loss: -49.1641 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x4dc985afc8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, validation_data=val_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-182-ddb04395b90c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "print('Accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Feature extraction started ***\n",
      "*** Feature extraction finished ***\n",
      "*** Feature extraction started ***\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-829acd17cd69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Get features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtsfel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_series_features_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdsArray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtsfel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_series_features_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdsArray1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Data\\envs\\Tensorflow\\lib\\site-packages\\tsfel\\feature_extraction\\calc_features.py\u001b[0m in \u001b[0;36mtime_series_features_extractor\u001b[1;34m(dict_features, signal_windows, fs, window_spliter, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m                 \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_window_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwind_sig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m                 \u001b[0mfeat_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeat_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Data\\envs\\Tensorflow\\lib\\site-packages\\tsfel\\feature_extraction\\calc_features.py\u001b[0m in \u001b[0;36mcalc_window_features\u001b[1;34m(dict_features, signal_window, fs, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m                 \u001b[1;31m# Name of each column to be concatenate with feature name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal_window\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m                     \u001b[0msignal_window\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignal_window\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m                 \u001b[0mheader_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignal_window\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Data\\envs\\Tensorflow\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    506\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mis_named_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m                     \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    509\u001b[0m                     \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Data\\envs\\Tensorflow\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[0;32m    525\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m         return _list_of_dict_to_arrays(\n\u001b[1;32m--> 527\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m         )\n\u001b[0;32m    529\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Data\\envs\\Tensorflow\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_list_of_dict_to_arrays\u001b[1;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[0msort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfast_unique_multiple_list_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m     \u001b[1;31m# assure that they are of the base dict class and not of derived\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.fast_unique_multiple_list_gen\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Program Data\\envs\\Tensorflow\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    635\u001b[0m     \"\"\"\n\u001b[0;32m    636\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m         \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m         \u001b[0msort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfast_unique_multiple_list_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "#@title Feature Extraction\n",
    "sylvan = 'D:\\\\desktop files\\\\weka\\\\SOFTX_2020_1-master\\\\notebooks\\\\sylvas.xlsx'\n",
    "\n",
    "googleSheet_name = \"Features_dev\"\n",
    "# Extract excel info\n",
    "cfg_file = tsfel.get_features_by_domain()\n",
    "\n",
    "# Get features\n",
    "X_train = tsfel.time_series_features_extractor(cfg_file, dsArray, fs=100)\n",
    "X_test = tsfel.time_series_features_extractor(cfg_file, dsArray1, fs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'X': array([ 0.627,  0.085,  0.018, ...,  0.442, -0.235, -0.328]), 'Y': array([ 0.097, -0.527, -0.119, ..., -0.19 , -0.069, -0.227]), 'Z': array([-1.105, -0.023,  0.332, ...,  0.562, -0.016,  0.042])}, array([5, 5, 5, ..., 5, 4, 4]))\n",
      "({'X': array([ 0.118,  0.299,  0.18 , ..., -0.65 , -0.611,  0.376]), 'Y': array([-0.3  ,  0.524,  0.038, ..., -0.9  , -0.086, -0.101]), 'Z': array([ 0.032, -0.279, -0.109, ...,  0.098, -0.126,  0.192])}, array([5, 5, 5, ..., 5, 5, 5]))\n",
      "({'X': array([-0.303, -0.043,  0.131, ...,  0.248,  0.256,  0.201]), 'Y': array([ 0.039,  0.159, -0.135, ...,  0.138, -0.303,  0.02 ]), 'Z': array([ 0.062, -0.072,  0.   , ...,  0.26 ,  0.072, -0.067])}, array([5, 4, 5, ..., 5, 5, 5]))\n",
      "({'X': array([ 2.920e-01,  1.460e-01,  2.100e-01,  1.400e-02,  1.730e-01,\n",
      "        4.470e-01,  1.390e-01, -2.550e-01, -3.330e-01, -7.060e-01,\n",
      "        3.730e-01, -1.190e-01, -1.590e-01,  1.500e-02, -7.590e-01,\n",
      "       -4.600e-02,  5.800e-02, -5.830e-01, -1.680e-01, -9.300e-02,\n",
      "       -9.100e-02,  2.590e-01, -2.630e-01,  9.400e-02, -1.330e-01,\n",
      "        1.910e-01,  1.000e-03,  3.250e-01, -4.300e-02, -4.770e-01,\n",
      "       -1.310e-01,  4.600e-02,  6.020e-01, -5.380e-01,  3.070e-01,\n",
      "        7.810e-01,  8.860e-01, -2.240e-01, -2.190e-01, -5.930e-01,\n",
      "        1.750e-01,  6.410e-01,  2.270e-01, -1.910e-01, -3.600e-02,\n",
      "        2.300e-02, -6.300e-02, -6.400e-02, -1.080e-01,  5.190e-01,\n",
      "        5.420e-01,  1.600e-01, -3.600e-01, -1.180e-01, -1.013e+00,\n",
      "       -1.180e-01, -3.360e-01, -1.780e-01,  6.900e-01,  5.300e-02,\n",
      "       -6.480e-01,  1.720e-01,  1.950e-01, -2.290e-01, -9.000e-03,\n",
      "        4.640e-01, -2.570e-01, -5.200e-02, -8.000e-01, -2.700e-02,\n",
      "        5.540e-01, -2.660e-01,  2.630e-01, -2.110e-01,  3.890e-01,\n",
      "        2.600e-02, -3.500e-02,  3.840e-01,  3.720e-01,  5.430e-01,\n",
      "        3.300e-02, -1.090e-01, -4.200e-02, -7.000e-02, -8.200e-02,\n",
      "       -9.960e-01,  1.100e-02,  1.830e-01, -1.720e-01,  5.400e-01,\n",
      "       -2.600e-01,  6.900e-02,  2.070e-01,  1.760e-01, -1.710e-01,\n",
      "       -4.750e-01, -5.230e-01, -8.590e-01,  8.400e-02,  2.940e-01,\n",
      "       -2.850e-01,  4.970e-01]), 'Y': array([-3.700e-02, -1.380e-01,  3.210e-01,  4.070e-01,  2.200e-02,\n",
      "        5.200e-02, -1.690e-01, -1.090e-01, -3.200e-02,  2.050e-01,\n",
      "       -3.000e-02,  3.020e-01, -6.700e-02, -2.060e-01, -1.310e-01,\n",
      "        1.190e-01,  2.450e-01, -1.970e-01,  2.060e-01, -1.490e-01,\n",
      "       -2.560e-01, -3.270e-01,  3.640e-01, -4.040e-01,  4.200e-01,\n",
      "        2.410e-01,  2.740e-01,  1.510e-01,  5.480e-01,  6.500e-02,\n",
      "        5.500e-02,  1.430e-01,  1.071e+00, -8.330e-01, -2.220e-01,\n",
      "        2.200e-02, -3.000e-03,  2.150e-01, -1.090e-01,  2.900e-01,\n",
      "        1.920e-01,  2.350e-01,  3.580e-01, -3.700e-02, -8.960e-01,\n",
      "        5.100e-02, -2.040e-01,  1.180e-01,  9.400e-02,  1.700e-02,\n",
      "       -4.210e-01,  4.330e-01,  4.130e-01,  2.330e-01, -1.151e+00,\n",
      "       -6.600e-02, -4.150e-01, -7.300e-02, -3.020e-01, -1.110e-01,\n",
      "       -7.540e-01,  2.980e-01, -9.900e-02,  6.940e-01, -2.980e-01,\n",
      "        5.030e-01, -6.360e-01, -1.000e-03, -7.500e-01,  4.580e-01,\n",
      "       -8.080e-01, -5.500e-02,  1.830e-01, -4.210e-01,  2.060e-01,\n",
      "       -1.400e-02, -2.300e-02,  2.700e-02, -3.200e-02, -7.700e-02,\n",
      "       -6.510e-01,  2.270e-01,  4.800e-02, -3.150e-01,  7.700e-02,\n",
      "       -7.710e-01,  6.830e-01, -2.410e-01, -1.076e+00,  4.450e-01,\n",
      "        4.180e-01, -3.620e-01,  4.600e-02, -2.880e-01, -1.090e-01,\n",
      "       -4.400e-01,  1.270e-01, -3.000e-02, -7.500e-02,  1.580e-01,\n",
      "       -6.700e-02,  4.860e-01]), 'Z': array([ 1.400e-02,  8.600e-02,  1.070e-01, -1.000e-02,  8.920e-01,\n",
      "       -1.100e-02,  9.800e-02, -2.900e-02,  3.000e-03, -7.100e-01,\n",
      "        5.400e-02,  3.700e-02,  4.500e-02,  2.000e-03, -1.158e+00,\n",
      "       -5.160e-01, -8.000e-03, -1.057e+00,  0.000e+00,  1.310e-01,\n",
      "        1.000e-03, -1.360e-01, -4.200e-02,  3.700e-02,  7.000e-03,\n",
      "        4.600e-02,  5.260e-01, -6.700e-02, -3.300e-02, -3.900e-02,\n",
      "       -2.000e-02,  6.600e-02, -1.455e+00,  6.050e-01, -7.500e-02,\n",
      "       -1.900e-02,  2.000e-03,  3.000e-03, -2.821e+00,  2.810e-01,\n",
      "        1.368e+00, -2.108e+00, -1.750e-01,  7.000e-03,  4.660e-01,\n",
      "        8.000e-03,  4.000e-02, -4.300e-02,  1.700e-02,  2.300e-02,\n",
      "       -3.710e-01,  1.100e-02,  6.000e-03, -3.700e-02,  1.075e+00,\n",
      "       -2.600e-02,  1.720e-01, -5.100e-02,  2.000e-02, -2.730e-01,\n",
      "       -8.870e-01,  4.000e-02, -4.200e-02, -2.810e-01,  2.600e-02,\n",
      "        6.100e-02,  1.997e+00, -3.100e-02,  3.082e+00,  3.100e-02,\n",
      "        8.560e-01, -1.500e-02, -3.900e-02,  2.230e-01,  7.150e-01,\n",
      "        6.100e-02,  3.000e-03,  3.480e-01,  4.000e-03, -2.720e-01,\n",
      "        4.860e-01, -8.500e-02, -4.500e-02,  2.700e-02,  1.420e-01,\n",
      "        1.340e-01, -2.332e+00, -3.900e-02,  3.332e+00, -7.160e-01,\n",
      "       -1.400e-02,  4.520e-01,  1.100e-02,  8.800e-02, -4.300e-02,\n",
      "        5.400e-02, -3.300e-02, -9.000e-03,  8.000e-03,  1.000e-02,\n",
      "        9.100e-02, -9.000e-02])}, array([5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "       5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5,\n",
      "       5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 4, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5,\n",
      "       5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 5, 6, 5, 5,\n",
      "       4, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5]))\n"
     ]
    }
   ],
   "source": [
    "#  A check of the dataset\n",
    "#for feature_batch, label_batch in train_ds.take(10):\n",
    "#    print('A batch of x:', feature_batch['X'])\n",
    "#tfds.list_builders()\n",
    "#\n",
    "dsArray = tfds.as_numpy(train_ds)\n",
    "dsArray1 = tfds.as_numpy(test_ds)\n",
    "\n",
    "#print(datasetArray)\n",
    "for x in dsArray:\n",
    "    print(x)b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
